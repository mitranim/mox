;;
## Data Notation

Our language's syntax is defined at two levels. The first level defines a
_data notation_: a set of rules for converting text to a nested structure of
data nodes. This level is not aware of any language specifics. The second
level is a programming language "syntax" defined in terms of the data
notation, in terms of data nodes, not in terms of characters.

This concept is fundamentally inspired by the idea, which has been demonstrated
and proven in practice, that programming languages greatly benefit from being
defined in terms of a foundational data notation, and NOT in terms of character
sequences, reserved keywords, or similar.

This is conceptually and syntactically similar to Clojure's EDN.
Unlike EDN, this is intended to represent a wider range of types,
with context-specific specialization in dialects.

Editors should implement a simplified syntax highlighter for the data
notation, independent of any language or configuration defined in it.
This could be useful for future DSLs.

This notation is defined in terms of Unicode characters, independently of any
text encoding. Specific parser implementations may restrict input to UTF-8
text. Specific language implementations may restrict literal strings to UTF-8.

This notation doesn't support commas and discourages unnecessary punctuation.
Statements, expressions, elements in data structures, and function arguments
should use only whitespace for separation.

### Specialization

The base data notation is intended to be _specialized_ into dialects.
Currently two dialects are planned: the language proper, and a config
data format similar to JSON. Each dialect defines its own semantics.
Dialects are allowed to slightly modify the parsing rules, and define
their own syntactic restrictions.

  data_notation ──┬── branch0: language
                  │
                  └── branch1: data_format

The data notation defines () [] {} as lists.
The language and the data format dedicate {} to dicts / structs.

Where syntax highlighting and auto-formatting also need to be
specialized, dialects should define their own file extensions.

### Types

The base data notation does not dictate the exact types of the data nodes.
The only requirements are that numeric types are decoded as something numeric,
comments / strings / symbols are parsed as something stringy, and () [] {} are
parsed as structures / groups containing the nodes between the opening and
closing delimiter.

Different dialects are likely to use different node types. A language parser is
likely to use specialized AST node types, while a generic data parser is likely
to use regular value types.
;;

;;;;;
## Comments

Comments begin and end with semicolon fences, and are always multi-line.
The fences must match. Fences are at least 2-semicolon long, but can be
extended indefinitely to allow nesting. The following are all valid:

  ;; comment ;;

  ;;; comment ;;;

  ;;;;
  comment
  ;;; embedded comment ;;;
  comment
  ;;;;

Comments may be used to generate documentation, following the conventions
established by Go, Swift, Rust. Comments should use Markdown. A comment
immediately followed by a named definition serves as the documentation for
that definition.

Parsers should have an option to preserve comments and attach them to data
nodes. This is needed for formatting and documentation generation, which are
common and extremely useful features.
;;;;;

;;
## Numbers

The notation defines integers and fractional numbers, with several radixes.
There is no restriction on number length / size, but actual implementations
may have their own limitations, such as not having arbitrary-precision
integers or fractionals.

The Ne+N exponent notation is not supported; it conflicts with number-symbol
adjacency. The exponent notation `NeN` is possible to support (`123e456`).

Letters in number literals must be lowercase for ease of reading.

Number literals may contain a leading `-` or `+`, which is treated as a special
case distinct from operators / symbols.

There is no support for numeric suffixes seen in some languages,
like `123ul` in C. Numeric literals are untyped, like in Go.

Numbers may contain `_` separators, but cannot begin with `_`.
;;

;; Binary ;;
0b01 0b01.01

;; Octal ;;
0o01234567 0o01234567.01234567

;; Hex ;;
0h0123456789abcdef 0h0123456789abcdef.0123456789abcdef

;; Decimal ;;
0123456789 0123456789.0123456789

;; Separators ;;
123_456_789

;;
### Number parsing restrictions

The contents of comments and strings are exempt from all restrictions below.

To avoid ambiguities and reserve space for future extensions, the parser must
reject letter characters immediately following a number, if they're not
valid digits for its radix. This automatically rejects unknown radix
specifiers, as they appear to be malformed decimals. This restriction
must also be applied immediately after a known radix specifier.

  0b012         -- Invalid: `2` is not a binary digit.
  0123456789a   -- Invalid: `a` is not a decimal digit.
  0123456789e10 -- Invalid: `e` is not a decimal digit.
  0b            -- Invalid: missing digit after radix.
  0b2           -- Invalid: `2` is not a binary digit.
  0o            -- Invalid: missing digit after radix.
  0o8           -- Invalid: `8` is not an octal digit.
  0h            -- Invalid: missing digit after radix.
  0hg           -- Invalid: `g` is not a hex digit.
  0x            -- Invalid: missing digit after radix.

Aside from letters, numbers may be immediately followed by any other node
defined in this data notation:

  10+20  -- Parsed like `10 + 20`.
  10[20] -- Parsed like `10 [20]`.
  10.len -- Parsed like `10 .len`.

The fractional dot must have digits on both sides,
otherwise it becomes a standalone symbol:

  1.2 -- Parsed as a fractional number.
  .1  -- Parsed as `. 1` where `.` is a symbol.
  1.  -- Parsed as `1 .` where `.` is a symbol.

### Characters

Character literals are currently not supported
because they're easily avodable.

One common approach is to define a function that takes a string
and returns a Unicode code point. Working Python example:

    ord("a") # 97

In a compiled language we could validate and evaluate this at compile time.
Thus, we avoid unnecessary syntax at no runtime cost. This approach is also
more flexible: you could define multiple functions, tailored for specific use
cases.

If we really wanted character literals, they could use otherwise dead
syntactic space in numeric literals, something like:

  0c_a -- Interpreted as 97 (lowercase "a").

...But string literals seem simpler.
;;

;;
## Strings

All string literals are multi-line. Apostrophe-like single quotes
are currently not supported and generate a parse error.

All string literals support indefinitely resizable fences, similar to comments.
The only restriction is that a fence cannot consist of exactly two quote chars,
because that would be ambiguous with an empty string literal.

All of the following are valid string literals:

  ""
  "str"
  """str"""
  """"str""""
  ``
  `str`
  ```str```
  ````str````

The following code is not what it seems:

  ""str"" -- Parsed like "" str "".
  ``str`` -- Parsed like `` str ``.

There's no special support for string interpolation. Languages defined in
terms of this notation may choose to implement some form of interpolation or
C-printf-like formatting via macros. The default recommended approach is to
provide a variadic function that collates a single string from its arguments:

  str.[one two three]
;;

;;
Double-quoted strings are multi-line, with indefinitely resizable fences,
and support character escape sequences.

Since code formatters need to be able to write code back as-is, a parser should
either store a reference to the source text in the resulting AST node, or treat
the string as raw without decoding the escapes.
;;
"
double-quoted string:
supports newlines and escapes: \t \n
"

;;
Backquoted strings are multi-line, with indefinitely resizable fences,
and are raw, without any escape sequences.
;;
`
backquoted string:
supports newlines, but not escapes: \t \n
`

;;
## Symbols

Symbols are sequences of unquoted characters, and may include
most of the printable non-whitespace ASCII characters:

  ABCDEFGHIJKLMNOPQRSTUVWXYZ
  abcdefghijklmnopqrstuvwxyz
  0123456789
  ~ ! @ # $ % ^ & * : < > ? / \ | = + - _ .

The set of allowed "operator-like" characters is likely to be extended later.

A symbol can never begin with a decimal digit.

The dot `.` is special: it terminates symbols. A leading dot takes precedence
over a trailing dot. Additionally, the restriction of not starting with a
decimal digit also applies after a dot, allowing numbers in dot-chains:

  one.two         -- Parsed like `one .two`
  one.two.three   -- Parsed like `one .two .three`
  one.123.two     -- Parsed like `one. 123 .two`
  one.123.456.two -- Parsed like `one. 123.456 two`

We distinguish "identifier characters" and "operator characters", but at the
base level of the data notation, symbols may contain arbitrary mixtures of
these characters. Distinct adjacent symbols must be separated by whitespace:

  - -- --- + ++ +++ = == === -- Distinct symbols.
  one+two                    -- Parsed as one symbol.
  one + two                  -- Parsed as three symbols.
  10+20+30                   -- Parsed as `10 +20 +30`: num, sym, sym.
  10 + 20 + 30               -- Parsed as num, sym, num, sym, num.

Specialized dialects may define additional restrictions
on the mixing of "identifier" and "operator" characters.

The base level of notation doesn't define any meaning for any symbols.
In practice, data decoders and encoders would use a slightly higher level
specialization of the notation, closer to JSON, with support for the values
`nil` `false` `true` and dictionaries via `{}`.
;;

;;
## Blocks

The base notation supports () [] {}, which are parsed as arbitrary sequences.
There are no special rules about their content. Languages defined in terms of
this notation may interpret them as specific data structures and impose their
own rules, such as using only one pair of delimiters.

That said, {} is generally intended for dicts / maps / structs.
;;
() [] {}
([10 "20"] {30 "40" (ident)})

;;
## Calling

The base data notation doesn't define a notation for function calls.
Languages are free to choose their own. The syntax highlighting for this
format should not attempt to detect "calls", leaving this to specific
languages.

That said, the actual language does have a specific call notation.
See `./lang.mox`.
;;

;; Dot-based unary call notation: ;;
some_func.some_arg

;; Infix call: ;;
10 + 20

;; Outside-block prefix call notation: ;;
some_func(arg arg arg)
some_func[arg arg arg]
some_func{arg arg arg}

;; Inside-block prefix call notation: ;;
(some_func arg arg arg)
[some_func arg arg arg]
{some_func arg arg arg}

;; Forced prefix call notation: ;;
some_func: arg arg arg
some_func! arg arg arg

;; Implicit prefix call notation: ;;
some_func arg arg arg

;; A language may require operators to follow the function call notation: ;;
+(10 20 30)
[+ 10 20 30]
+ : 10 20 30

;;
A language is free to define operators as prefix, infix, or postfix,
with its own precedence and grouping rules.
;;
10 + 20 - 30
@ref += 10

;;
Because this notation doesn't support commas and discourages unnecessary
punctuation, unary operators might require grouping to disambiguate them from
binary infix:
;;
10 - (- 20)

;;
## Serialization

This notation can be specialized to describe simple data, like JSON,
by treating `{}` as dicts and providing `nil` `false` `true`.
This should be used for config files.
;;
nil
false
true
[10 20 30]
["value0" "value1" "value2"]
{"key0" 10 "key1" 20 "key2" 30}

;;
However, it's much more flexible than that.

JSON is self-describing for dynamic languages when using a small number of
built-in types. For typed languages, JSON is not self-describing, and lacks
the fluency to annotate values with types.

This notation may be specialized to support explicit typing with "constructor"
calls. This allows a much richer set of types compared to JSON. In this sense,
our notation is closer to XML than JSON. But compared to XML, it's MUCH easier
to write and read, and has a richer set of primitive building blocks, not just
strings.

Example of such specialization below. Rust users may spot a similarity to RON,
but this notation is lighter because it avoids `:` and `,`.
;;

SomeStructType{
  some_field    `some_value`
  another_field AnotherType[10 20 30]
}

;;
A specialization could also have implicit default types for some structures,
allowing to omit type specifiers:
;;

;; Various ways to represent lists ;;
[`value0` `value1` `value2`]
List[`value0` `value1` `value2`]
[List `value0` `value1` `value2`]
:[`value0` `value1` `value2`]
[: `value0` `value1` `value2`]

;; Various ways to represent sets ;;
(`value0` `value1` `value2`)
Set(`value0` `value1` `value2`)
(Set `value0` `value1` `value2`)

;; Various ways to represent dicts ;;
{`key0` 123 `key1` 456 `key2` 789}
Dict{`key0` 123 `key1` 456 `key2` 789}
::{`key0` 123 `key1` 456 `key2` 789}

;; Datetime ;;
Time(`1234-12-34T01:02:03Z`)

;; XML ;;
html[
  body[
    {attr0 `value0` attr1 123}

    span[
      {attr2 `value2` data-attr 456}

      namespace:element[`text_node`]
    ]
  ]
]
