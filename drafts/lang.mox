;;
## Overview

Preliminary sketches and notes for a hypothetical programming language which
doesn't exist yet, but needs to. Basically a long wishlist from the language
user perspective.

This doc covers syntax and expressions first because that was fun to write
and because I think syntax is fundamentally broken in most languages
(over-specialized) and needs fixing (un-specializing).

But this is REALLY about fixing the execution model.

The core idea is to have a language which doesn't force a split between AOT
compilation vs JIT execution, but does both and lets you freely mix and match,
and MOVE the line between where and when you want particular code to run. In
development you should be able to mostly use the JIT mode for faster iteration,
in production you will AOT compile, and you should be able to simply import the
compiler to load plugins and mods at app runtime, and JIT + link them against
your program to run at native speed. Baffling that no major language does this
natively. (Dylib hacks are neither native nor ergonomic.) This JIT focus should
also make it almost self-bootstrappable.

It's kinda like saying that the compiler core should be small and fixed, and
modules should be able to plug into the compiler on the fly, modify behaviors
like codegen, install new behaviors, instruct the compiler how to compile them,
and so on.

The JIT+AOT combo means you should be able to prototype very rapidly, then build
up, without having to rewrite in another language for performance. To make this
really practical, we'll need powerful type inference.

## Influences

This document has many influences.

Years of suffering from poor designs and thinking about better designs.

Many, many programming languages. Non-exhaustive list, ordered aesthetically:
- Go
- Nim
- Zig
- Jai
- Odin
- Rust
- Swift
- Forth
- POP-11
- Erlang
- Haskell
- Smalltalk
- Red / Rebol
- Lambda calculus
- Clojure and other Lisps
- https://paulgraham.com/hundred.html

Honorary shoutout to anti-inspirational systems such as C++ and Java.
Discovering what to _avoid_ is quite valuable.

In languages, we shouldn't settle for "the right tool for the job".
In many jobs, there are many tools and some of them are better than
others. But when your job has you reaching for a language, it should
be possible to define ONE language which is right for every job.
Further specialization should be done with libraries.

We need one "final" language, which should:
- Be right for every job.
- Be usable for systems programming, mainframes, games,
  web apps, scripting, shells, and more.
- Be high level and low level both.
- Be good for fast iteration (low latency).
- Have AOT compilation to machine code.
- Have JIT compilation and hot module swapping.
- Interchange JIT and AOT flexibly.
- Hot-load+JIT plugins and mods in production.
- Compile for GPUs.
- Compile for browsers (to plain clean JS).
- Deduplicate related concepts. (See [#scopes_are_structs].)
- Have an extemely simple and easy-to-type syntax.

## Syntax and AST

The language is defined in terms of its data notation, which is similar
to Clojure's EDN. It's described in more detail in `./data_notation.mox`.
In short, it supports:
- Comments
- Numbers
- Strings
- Symbols
- Structures: () [] {} (distinct types)

Notable differences from Clojure's EDN and AST:
- No support for `:keyword` symbols.
- No support for reader macros. (Don't need them.)
- Symbols are second-class and not intended for runtime use by apps.
- While {} is always used for key-value mapping, it might actually be
  parsed as a list, if we end up using duplicate keys for overloads.
- {} grudgingly requires `=` in key-values; see [#structs] below.

Delimiter purposes:
- () is for app-compile-time sequences: blocks, groups.
- [] is for app-execution-time sequences.
- {} is for dicts and structs.

The AST won't be homoiconic; Clojure achieved it by wasting memory, as is the
JVM way; it reserves a metadata field in a hefty variety of built-in data
structures. That said, we _are_ spiritually close to homoiconic languages, by
virtue of defining the "syntax" of the language in terms of data structures,
not text.

Describing a language in a simple, fixed data notation, and avoiding keywords,
is an extremely powerful idea which greatly simplifies the syntax, parsing, and
AST manipulation, allows for very flexible DSLs, and makes learning easier for
many. This has been well-explored in Lisps.

The data notation syntax is fixed. The parser is implemented once. Users learn
the notation, and _think_ of language features in terms of data. When adding
new language features, even ones very syntactically distinct, we define them in
terms of existing data nodes, with no changes to the parser. Users also learn
new features in terms of data. Though we aim to make the compiler extensible
on the fly, the AST and expression structure is the one unbendable rule.

In one sense, the syntax of the language is frozen forever. In another sense,
the syntax is infinitely flexible, because our data notation can represent a
huge range of "shapes". Code can be given any shape suitable for the task.

Our notation is whitespace-insensitive, except a few edge cases where atomic
tokens must be separated by whitespace or other tokens.

There are no statements, only expressions.

There are no keywords, no reserved names, and no special characters
beyond the data notation.

Configuration files would be written in a specialized version of the data
notation, with support for `{}` dicts, `nil`, `false`, `true`, once again
similar to the EDN. See `./example_config.mox`.

While the syntactic flexibility is extreme, the data notation also places
a lower bound on the expressivity: the underlying structures are always
recognizable.

Unlike Lisps, this language aims to have distinct visual code shapes for
distinct concepts. Glancing at a code should vaguely tell you the shape
of the control flow. Declarations should be readily distinguishable from
usage. Sequences should be distinguishable from key-value maps at a glance.

However, _like_ Lisps, we aim to define special "syntactic" constructs entirely
in library code. They are not part of the data notation, and the compiler core
should have no knowledge of them.
;;

;; Symbols ;;
nil
false
true
some_identifier
!@#$%^&* ;; Custom operator ;;

;; List literal (contextual) ;;
[10 20 30]

;; Struct literal (contextual) ;;
{
  one = 10
  two = 20
}

;; Dict literal (contextual) ;;
{
  `one` = 10
  `two` = 20
}

;;
## Language features and compile-time execution

Every compiler is an interpreter. Any C file is a dynamically typed script
sequentially executed by the compiler at compile time. The consideration of
eventual runtime semantics and generation of runnable machine code is a side
effect of that execution.

C is just an example. Any single-pass compiler running over source code is
interpreting that code on the fly as a script. Multi-pass compilers are more
complex, but the core principle of our argument holds for them as well.

Every piece of code has 2 distinct sets of semantics:
- Interpretation semantics, which modify compiler state.
- Execution semantics, which modify the resulting program's state.

When a compiler sees a variable declaration, it interpets that as a command to
perform whatever static checks it supports, then to modify the current scope,
allocate stack space if needed, and so on. The same idea applies to function
declarations, type declarations, and basically every language feature.

We shouldn't keep this power locked inside compilers. And compilers shouldn't
be black boxes full of special cases. Instead, we should take the notion that
compilers are interpreters to its logical conclusion, embrace compile-time
execution of arbitrary code, and define the entire language by _scripting the
compiler_ in the standard library code.

Special features supported by the compiler may include the following.
This is just a rough sketch:
- Scopes.
- Some way of declaring names (putting them in scope).
- Some way of defining types.
- Some way of defining functions, or at least runnable code.
- Compile-time execution of arbitrary code.
  - Cold code should be interpreted (faster for run-once code).
  - Hot code should be JIT-compiled.

The compiler gives us access to its runtime state, which includes the AST,
scopes, type information, and more. The compiler provides a special
pseudo-module which gives programs access to the state and definitions.
Furthermore, the compiler provides ways of _manipulating_ its state, and
extend it with new behaviors, effectively scripting it with plugins.

The compiler allows us to mark arbitrary procedures, or rather arbitrary names
referring to procedures, as "immediate". When the compiler encounters calls to
those procedures, it invokes them immediately. The inputs are either AST nodes
or static values, depending on a procedure's definition, as well as additional
context for that location in the code, such as current scope, expected types,
and more.

The core type-definition primitive provided by the compiler could be just
"a chunk of memory with N bytes". This might be sufficient for the stdlib,
which uses it to define primitive types such as integers, then progressively
more complex types. If the notion of complex structures such as "structs"
and "slices" can be kept outside the compiler's core, it should be.

The standard library bootstraps itself from core primitives, but does not
force this on user code. It provides higher-level features such as a rich
set of built-in types and functions operating on them. But the standard
library is entirely optional. A program targeting a tiny embedded chip
may choose to bootstrap itself using different primitives.

In summary:
- The only special syntax is how to invoke procedures.
- The compiler provides only what's needed for bootstrapping.
- The ability to script the compiler unlocks everything else.
;;

;;
### Immediate procs

TLDR: aim to support arbitrary compile-time computations via immediate procs.
This is also known as "comptime" / "static" / "constant" / "comtime", etc.
The opposite term is "deferred" / "runtime" / "execution time".

Immediate procs have many use cases:
- Manipulating AST / scope / compiler state / etc.
- Pre-calculating constant values.
- Enabling conditional compilation.
- Performing additional validation.

The first group is usually called "macros",
and tends to be used for syntactic features.

The second group is useful for precalculating constant values
at compile time to avoid the cost at program execution time.

For any proc declaration, it should be possible to make it "immediate", marking
it for automatic immediate compile-time execution. Any call to that proc is
invoked by the compiler immediately during compilation. The inputs have to
be fully available at compile time; this means they can be either AST nodes,
or regular types defined by the language, but constant: either literal, or
computable by calling other procs at compile time.

In immediate procs which manipulate the program itself ("macros"), it should
be possible to access any part of the compiler state, as well as the local
context: operands as AST nodes, local scope, and so on.

It should also be possible to decide at any callsite when to run a given
expression, by marking that expression as "immediate". The compiler computes
the whole expression, invoking every proc at compile time, and stores the
constant result. This is similar to `static` in Nim.

Immediate execution has a compile-time cost. When iterating on a program,
developers shouldn't have to sit through repeated recalculations of values
which might not even be used. It should be possible to make this conditional.
Some proc declarations or callsites could specify that the given computation
is immediate in release builds, but deferred (runtime) in dev / debug builds.
Semantics should be preserved: such immediate computations should be memoized
and not repeated. It seems likely that memoized runtime computations should be
the preferred default for dev / debug builds, since this reduces waiting time
while keeping the total computation time similar.

One example of the usefulness of arbitrary compile-time computations is
converting a constant string into a regex state machine. In Go, this can
be done only at program execution time, and naive code might run this
repeatedly and redundantly, instead of storing the result. In a language
with arbitrary compile-time computations, this can be easily avoided.

More generally, string literals are used to embed many different data formats
into programs: literal URLs, literal SQL, literal JSON, and so on. In langs
without compile-time calculations, all the parsing is done at runtime, and
libraries have to be carefully written to cache the results to avoid repeated
parsing. We shouldn't have to cope with that.

Another use case is conversion between structure formats. For example, since
Haskell doesn't have dict / map literals, Haskell maps are constructed via
`fromList [(key, val), (key, val)]` at a runtime cost. The cost could in
principle be eliminated by special-casing this in the compiler, but any
self-respecting language should make it possible for library code, outside
of the compiler core, to do that.
;;

;;
## Proc arity and call syntax

Many languages have "operators", "functions", "methods", and "struct fields" as
distinct concepts. We unify them into one. This document calls them "procs" for
brevity. The name is not important.

We also unify arities and generalize proc inputs. Many languages pretend to have
different arities for procs. Usually, what this _really_ means is that every
proc takes one input which is a heterogeneous tuple, and you can never pass
a _non-tuple_ as an input to any procedure.

Some languages support returning "multiple values" from a procedure.
Just like "multiple inputs" are in fact a heterogeneous tuple, the notion
of "multiple return parameters" is also simply a heterogeneous tuple.

We unify these concepts. There is exactly one proc arity: unary. Every proc
takes one input and returns one output. We picture this with `A -> B`.
The input `A` and output `B` may be of any type, not just tuples!

We have three call syntaxes:
- Nullary evaluation
- Unary invocation via `.`
- Binary invocation via operators

Implicit nullary evaluation is distinct from explicit nullary calls
present in other languages. See [#nullary_evaluation] below.

We have the following call patterns, which are explained below.
`S` stands for a hardcoded literal symbol, while `A B C` stand
for arbitrary input and output types.

  S     -> A
  A     -> B
  A . S -> B
  S . A -> B
  A . B -> C
  A S B -> C    -- Where `S` must be an operator.

The call syntaxes pictured below aim to satisfy specific requirements:
- Support several ordering styles:
  - Unary `A S` (postfix, required for field getters).
  - Unary `S A` (prefix, required for sanity reasons).
  - Unary `A B` (not required, but comes out of `S A`).
  - Binary `A S B` (required for operator chaining).
  - Allow using `S` in any position when unambiguous.
- Easy to type.
- Light appearance.
- Avoid having to separate atomic tokens with `,` or `;`. The following must
  be valid syntax for a list with 3 elements: `[one two three]`. Our data
  notation forbids `,` and `;`, making this a hard requirement.
- Totally unambiguous; calls must be visually clear without hidden context,
  such what the types are, or what is the proc arity. This makes correct
  syntax highlighting of calls possible (in stark contrast with Rebol).

So here's the current plan. In these examples, `?` stands for any operator.
The dot is special. Whitespace is ignored.
- Operator binary infix: `arg_0 ? arg_1`
- Operator unary postfix: `arg_name.?`
- Operator unary prefix: `?.arg_name`
- Identifier unary postfix: `arg_name.proc_name`
- Identifier unary prefix: `proc_name.(arg_name)`

The big deal (really!) is we don't need the `:,;` separators / joiners in
_other_ parts of the code, such as literal data structures:

  [10 20 30]

  [
    10 + 20    -- elem 0
    30 + 40    -- elem 1
    50 + 60    -- elem 2
  ]

When rapidly writing and editing code, the cost of separators / joiners adds up
real fast. Most programmers seem to be oblivious to this. My hypothesis is that
most folks either haven't been spoiled by languages without this issue, or
haven't paid enough attention to how much time is wasted fixing typos involving
these characters.

Like Lisps, we use the same call syntax for runtime and compile-time procs.
Our heavy reliance on implementing language features as immediate procs makes
this unavoidable. Immediacy is attached to proc declarations.

We support infix and postfix because many complex expressions, such as long
data transformation pipelines, are nicer to write in a chainable style.

Some languages, notably Lisps, have only the prefix call notation, and end up
inventing pipeline macros, such as Clojure's `->` and `->>`. Users end up
having to convert code between two styles when adding or removing steps.
Having native infix and postfix spares us from that.
;;

;;
### Unary calls

Unary calls involve the special operator `.`, which is binary, with a higher
precedence than other infix operators. It allows the following call patterns
(whitespace-insensitive):

  S.S -> A
  A.S -> B
  S.A -> B
  A.B -> C

This is the only unary call syntax we support.

The pattern `S.S -> A` means: both operands are literal symbols, the LHS
references the input, the RHS references a named procedure applicable to
this input type.

The patterns `A.S -> B` and `S.A -> B` mean: `S` is a literal symbol, which
references a procedure of the type `A -> B`.

The pattern `A.S -> B` also represents namespacing. It allows for struct field
getters and module namespacing with a single mechanism. A field is essentially
a reference to that field's address. Reading a field dereferences the address.
Writing a field stores a value at that address. Since we intend to conceptually
unify modules with structs (see [#structs_are_modules]), the same mechanism
works for procs and vars namespaced via modules. See also: [#struct_fields],
[#variables], [#refs].

The unification of fields and procs allows switching between built-in plain
field getters and custom procedural getters with no refactoring.

The pattern `A.B -> C` means: the current scope must have an _anonymous_
procedure of the type `A -> B -> C`, which takes an operand `A` and returns a
procedure `B -> C`. This makes any value of type `A` callable with a value of
type `B`. Any type can be made "callable" this way. However, because such calls
tend to be illegible, this should be mostly avoided. We may end up not
supporting this at all.

Partial application of `A -> B -> C` involves returning a closure. Since we aim
for speed, such closures need to be either entirely avoided in machine code, or
stack-allocated, similarly to inline closures in Rust, C++, and C extensions.

Example unary calls (with an intermediary `.` binary call).
All are whitespace-insensitive.
;;
some_struct.some_field ;; Field ref; see [#refs]. ;;
some_int.to_string     ;; Integer to ASCII. ;;
to_string.(some_int)   ;; Integer to ASCII; grouping "escapes" a symbol. ;;
to_string.123          ;; Integer to ASCII. ;;
some_string.len        ;; Byte count or Unicode grapheme count. ;;
`some_string`.len      ;; Byte count or Unicode grapheme count. ;;
len.`some_string`      ;; Byte count or Unicode grapheme count. ;;
some_string.10         ;; Get byte at index 10. ;;
some_string.(ind)      ;; Get byte at index `ind`. ;;
`some_string`.10       ;; Get byte at index 10. ;;
log.`some_string`      ;; Print string. ;;
`some_string`.log      ;; Print string. ;;
log.(some_string)      ;; Print some string. ;;
log.[10 20 30]         ;; Print multiple numbers. ;;

;;
This syntax is also used for unary operators.
Without a dot, operators are binary; see below.
;;
some_bool.! ;; Boolean negation. ;;
some_num.-  ;; Numeric negation. ;;
some_var.&  ;; Taking a reference. ;;
some_ref.*  ;; Dereferencing. ;;

;;
Void procs, like `void(void)` in C, are actually `Nil -> Nil`.
They're invoked by passing nil:
;;
some_void_proc.()
nil.some_void_proc

;;
### Complex inputs and outputs

For complex inputs-outputs, we encourage _structures_. Complex homogeneous
inputs are represented with arrays or lists. Complex heterogeneous inputs
are represented with inline anonymous structs, known as named parameters
and named arguments. See the section [#named_parameters]. Ditto for outputs.
;;

;; Homogeneous structured input (list): ;;
some_proc.[10 20 30]

;; Heterogeneous structured input (struct): ;;
some_proc.{
  one = 10
  two = `some_string`
}

;;
### Binary infix calls

Symbols which look like operators (see `./data_notation.mox`)
are used for binary infix calls:

  10 + 20

Some special rules:
- The dot takes precedence: `10.-` is an unary call (returning `-10`).
- "Grouping" an operator takes its value: `(+)` is a proc value.

The grouping rule is very similar to Haskell. However, unlike Haskell,
we don't need to support grouped _unary_ operators such as `(- num)`,
since the dot does that already: `num.-`.

Operator symbols (see `./data_notation.mox`) not adjacent to dots, and not
grouped in parentheses, are always binary infix calls. Since all procs are
actually unary, this requires the following application order:
- Operator must refer to proc with type `A -> B -> C`.
- Apply to left operand, returning `B -> C` (inlined closure).
- Apply to right operand, returning `C`.
;;

;;
### Precedence

This is subject to revision, but here are
the planned precedence levels and rules:

- 0 -- dot operator `.`
- 1 -- pragma operator `:`
- 2 -- most operators
- 3 -- proc operator `->`
- 4 -- assignment operators such as `=`

Newly declared operators get precedence level 2 by default.
We also make it possible to change the level with a pragma,
but only the dot can be at level 0. We may end up expanding
the set of levels in the future; assignment operators will
always have the lowest precedence.

The dot-0 rule is important for expression grouping, as having a different dot
precedence would change how adjacent forms are grouped or not grouped into
macro-expressions. The other precedence rules affect only internal grouping
inside each macro-expression. This means they don't affect the separation of
macro-expressions from each other.

Operators can be left- or right-associative. Left associativity is the default.
Right associativity is opt-in, and used for a few built-ins.

Arithmetic operators are likely to have the exact same precedence.
See the code below. In many languages, it would become:

  10 - (20 / 30) + (40 * 50)

But here, it becomes:

  (((10 - 20) / 30) + 40) * 50
;;
10 - 20 / 30 + 40 * 50

;; Due to dot precedence, the following expressions are equivalent: ;;
12.34 - 56.78 .trunc
12.34 - (56.78.trunc)

;; For unary calls on results of binary expressions, use grouping: ;;
(12.34 - 56.78).trunc

;; Or simply split up: ;;
val = 12.34 - 56.78
_   = val.trunc

;;
Our simple expression grouping rules allow to avoid
element separator characters in list literals:
;;
[
  10 + 20 ;; First expression = first element. ;;
  30 + 40 ;; Second expression = second element. ;;
]

;;
The same goes for dict literals, which are alternations of keys and values
where values are arbitrary expressions. Because grouping is unambiguous,
key-value pairs do not require separators:
;;
{
  one = 10 + 20 ;; First field. ;;
  two = 30 + 40 ;; Second field. ;;
}

;; The following is valid but discouraged: ;;
(10.+).20               ;; use `10 + 20` ;;
(one.=).((two.+).three) ;; use `one = (two + three)` ;;

;;
### Currying

All "binary" procs, including all infix operators, are implemented in the
Haskell curried style `A -> B -> C`, where the outer proc takes `A` and
returns the inner proc `B -> C`. Thus, an infix call `10 + 20` is actually
two calls: `(10.+).(20)`.

This isn't limited to operators. Other common "binary" procs include:
- Postfix transformation chaining: `list.filter.(not_zero).foldz.(+)`.
- Interface conformance: `Self -> A -> B`.

The example below assumes that the current scope has a certain proc bound to
the symbol `map`, whose type is `A.List -> (A -> B) -> B.List`, with a generic
implementation that works for any `A.List`.
;;
some_list.map.(val -> val * 2)

;; Which is parsed equivalently to: ;;
some_list . map . (val -> val * 2)

;; ...And executed equivalently to: ;;
(some_list.map).(val -> val * 2)

;; Transformation chain: ;;
some_list
  .map.    (val -> val / 2)
  .filter. (val -> val % 2)
  .foldz.  (+)

;;
### Declaration and assignment

All declarations and assignments are done with the operator `=`.
This is used for constants / variables / struct fields / procs / types.
;;

;; Identifier declarations are straightforward: ;;
some_const = `some_val`
some_proc = val -> val * 2
Some_type = Int.typedef

;; Declaring an "operator" requires "escaping" it with parens: ;;
(!) = val -> val.not
(**) = one -> two -> one.pow.(two)

;;
By default, `=` declares constants. Mutability is declared by using the pragma
`mut` on the declared symbol. Mutable variables can be reassigned by reference.
See [#refs].
;;
mut : some_var = 10
some_var.& = 20
some_var ;; 20 ;;

;; Reassignment could use a dedicated operator if needed: ;;
some_var <- 30

;;
Initializing to "zero" or using the default constructor,
if any, is done by "assigning a type" to a variable:
;;
zero_int = Int
zero_string = Str
zero_struct = Some_struct

;;
While the operator `=` is likely to be intrinsically provided by the compiler,
it is otherwise NOT a syntactic special case. It's simply a pre-declared
operator marked as "immediate". Library or user code can define its own `=`,
shadowing the built-in. We intend to define other assignment operators in
standard library code, as immediate procs.

Procs such as `=` follow the regular operator precedence rules, in accordance
with their precedence and associativity. The example below also uses the pragma
operator `:`, which is also an immediate proc provided by the compiler. See
[#pragmas] below. Neither of these operators is a syntactic special case.
;;

;;
## Proc declarations

Proc syntax is provisional and subject to change. However, one thing is set in
stone: the proc body is either an expression, or a group of expressions, where
the last value is implicitly returned. Void procs take and return nil.

If the type signature is not provided, the input and
output types are inferred, similarly to Haskell.

Types can be specified with the pragma operator `:`.
See the section [#pragmas].

The operator `->` can define proc type signatures _and_ proc implementations.
Disambiguation rule: is the LHS ends with an Upper-titled symbol, that symbol
must refer to a known type in scope, the RHS must also do so, and `->` returns
a proc type signature. Otherwise, the LHS must end with a lower-titled symbol,
which is a parameter name; the RHS is the proc body, and `->` returns a proc.

The operator `->` is right-associative and has
the second-lowest precedence, just above `=`.

Unlike other languages, where such operators are syntactic special cases
in the parser, we define all these operators as immediate procs, even when
they're provided by the compiler rather than defined in library code.
;;

;; Proc type signatures: ;;
Str -> Int
Str -> Int -> Str
Str -> Int -> Str -> Int

;; Proc implementation: ;;
double = val -> val * 2
double.10

;;
Right associativity allows to chain `->` after each other.
This defines a "binary" proc, which can be dot-chained:
;;
add2 = one -> two -> one + two
10.add2.20

;;
Procs can be bound to identifiers or operators. When declaring an operator,
its name needs to be "escaped", similarly to Haskell, and similarly to
taking an operator's value:
;;
(+-+) = one -> two -> one + two - two + one
10 +-+ 20

;; Escaping an operator to take its value: ;;
weird_proc = (+-+)

;;
Proc types can be attached to proc declarations with the pragma operator `:`.
Its usage is similar to type declarations in many other languages, but it's not
a syntactic special case. See [#pragmas].
;;
(Int -> Int) : double = val -> val * 2

;; Can be placed on a separate line; note the trailing colon: ;;
(Int -> Int) :
double = val -> val * 2

;;
Procs often grow large, complex, and acquire many parameters. Sometimes you're
better off splitting them, but sometimes this is justified. The recommended
solution (the only one supported, really) is to use named parameters, in the
form of inline structs.
;;
complex_proc = {
  arg0 = Type0
  arg1 = Type1
  arg2 = Type2
} -> (
  arg0.complex_operation
  arg1.complex_operation
  arg2.complex_operation
)

;;
To provide a default parameter value, replace `= Some_type`
with `= some_default_value`:
;;
fopen = {
  path = Str
  mode = `r`
} -> (
  ;; ... perform a syscall ;;
)

;; Inline modifiers and types: ;;
some_proc = mut:Int:one -> const:Str:two -> one.format.(two)

;;
## Structs

Struct definitions and literals use `{}`.

Declaration _and_ assignment uses `=` between keys and values
for total consistency between:
- Field declaration in struct type definitions.
- Field assignment in struct literals.
- Variable declaration and assignment in scopes.

This is done for several reasons:
- Refactoring. It's not uncommon to factor out inline procedural code
  into structured types, or perhaps vice versa.
- Lower learning curve. Beginners intuitively feel that scopes (blocks)
  and structs are very similar, and different syntax trips them up.
- Total visual consistency.
- Easily correct syntax highlighting.

See [#scopes_are_structs] below.

Sample type definition:
;;
Person = struct.{
  id    = Int
  name  = Str
  email = Str
}

;; Sample construction: ;;
person = Person.{
  id    = 10
  name  = `one`
  email = `two@three.four`
}

;; Type can be inferred on either LHS or RHS of `=`: ;;
Person : person = {
  id    = 10
  name  = `one`
  email = `two@three.four`
}

;;
For variables _and_ struct fields, "assigning a type" is defined as
zeroing that variable or field by default. To provide another default,
replace a type with a value:
;;
Post = struct.{
  slug   = Str  ;; default zero ;;
  public = true ;; default non-zero ;;
  listed = true ;; default non-zero ;;
}

;; Accessible by URL (`public = true`) but no longer listed: ;;
post = Post.{
  slug   = `some_post`
  listed = false
}

;;
### Scopes are structs

This is a bit tentative, but conceptually, we aim to unify structs, scopes,
blocks, modules, because they're really just structs. It's structs all the
way down. As a result, constants and variables are just struct fields in
their respective scopes / blocks / modules.

To drive this home, and make learning and refactoring easier, we unify the
declaration / assignment syntax for all. See [#declaration_and_assignment].
;;

;;
One of the weirder implications is that we might be able to run a block
and "snapshot" it as a struct. The compiler infers the structure.

This may not often be useful for sub-scopes. But this is exactly what happens
with modules. Each module is evaluated and snapshotted as a struct. So there is
no reason not to allow this for sub-scopes.
;;
block = struct.(
  ;; Variable = public field. ;;
  pub:mut:one = 10

  ;; Variable = public field. ;;
  pub:two = one + 20

  ;; Variable = private field. ;;
  three = one + two

  ;; Side effect, runs exactly once, no field. ;;
  stdout.println.[`intermediary value: ` one]
)
stdout.println.(block.one) ;; 10 ;;
stdout.println.(block.two) ;; 30 ;;

;; Since we made one of the "fields" mutable, we can reassign it too: ;;
block.one.& = 40

;;
The above is distinct from _blocks as expressions_. It's often convenient to
execute a sub-scope with intermediary variables and grab the output. Many
languages support treating blocks as expressions. This only gets the result
value, not the whole block-struct.
;;
result = do.(
  one = 10
  two = 20
  one + two
)
log.(result) ;; 30 ;;

;;
### Modules are structs

Modules are blocks, and blocks are structs. Modules are structs. QED.

This could simplify the implementation: no special treatment for module root.

Additionally, we want to support the ability to "inherit" from a module and
override some of its procs and constants. This should be doable by "including"
all content of one struct in another, then adding various overrides.
See [#module_level_overrides].
;;
include.`some_other_module`
some_proc = some_override_proc

;;
Supporting this for modules automatically makes this available
for sub-scopes / blocks, which are also modules, after all:
;;
mod = struct.(
  include.`some_other_module`
  some_proc = some_override_proc
)
mod.some_proc

;;
In actual struct and dict literals, "include" might be emulated with a special
field name, such as `*`. It splurges the content of another struct or dict into
this one:
;;
some_struct = {
  (*)        = another_struct ;; Splurge all fields from `another_struct`. ;;
  some_field = `some_override`
}
some_dict = {
  (*)             = another_dict ;; Splurge all fields from `another_dict`. ;;
  `another_field` = nil          ;; Exclude this field. ;;
  `some_field`    = `some_override`
}

;;
### Struct fields

We aim to unify structs with scopes, and struct fields with variables and procs.

Struct fields are abstractions. Each field is effectively an offset in a tuple.

A struct field "get" is equivalent to invoking a proc `A.S -> B`, where A is
a struct type, `S` is a hardcoded symbol, and `B` is the field's value.

A struct field "set" writes a value to the memory at an offset. In principle
we could implement this by making fields "callable", so that the following
would set a field:

  some_struct.some_field.(some_value)

...But that would be inconsistent with variables, where this approach seems
impractical. In both cases, it's syntactically ambiguous.

Struct fields and variables are _addressable_.
Consider this common operation:

  outer.middle.inner.some_field = some_value

The solution involves [#refs]. Accessing a struct field via its proc actually
returns a ref: `A.S -> B.Ref`. However, this particular ref type is shy: it
implicitly dereferences unless the caller _really_ wants a ref and knows what
to do with it. Two procs which really know how to deal with refs are the dot
operator `.` and the assignment operator `=`.

The dot knows how to chain refs:

  outer.middle.inner.some_field  -- ref all the way to outer memory

`=` knows about refs, but rejects assignment to shy refs.
This is an error because `=` doesn't like it:

  outer.middle.inner.some_field = another_value

`=` likes assertive refs. The hypothetical immediate proc `&` performs a type
conversion from a shy, retreating ref into a bold, eager ref ready for
mutating assignment:

  outer.middle.inner.some_field.& = another_value
;;

;; Reading a field: ;;
some_struct.some_field

;; Writing a field: ;;
some_struct.some_field.& = `some_value`

;;
### Struct virtual fields (procedural getters)

Struct fields are abstractions defined with procs. What naturally follows is
that a field needn't be stored in memory. Simply defining a getter proc gives
a struct type another field.

Many languages have special syntactic support for field / property getters.
By unifying fields and procs, we get this for free, with no new concepts.

Because we also unify scopes with structs and variables with struct fields,
virtual variables should also be possible, by declaring an immediate proc
`S -> A` where S is any hardcoded symbol, which could do arbitrary things
to define a requested variable on the fly. This is different from struct
virtual fields which normally associate with a hardcoded field name, and
not demonstrated here.

The example "getter" below computes a virtual field from other fields.

The conditional syntax is provisional. See [#conditionals].
;;
(Person -> Str) :
guess_name = val -> case.{
  (val.name  != ``) = val.name
  (val.slug  != ``) = (`@` + val.slug)
  (val.email != ``) = val.email
  _                 = `(anonymous)`
}

;; Usage is identical to struct fields: ;;
some_person.guess_name

;; Using C-style ternary condition expressions: ;;
(Person -> Str) :
guess_name = val ->
  (val.name  != ``) ? val.name         !
  (val.slug  != ``) ? (`@` + val.slug) !
  (val.email != ``) ? val.email        !
  `(anonymous)`

;;
Shorter version which assumes that `Str` is implicitly convertible to `Bool`
and only zero-length strings are falsy. See [#truthiness]. Alternatively,
maybe it assumes that the operator `|` is defined on `Str`.
;;
guess_name = val -> or.(
  val.name
  and.(val.slug (`@` + val.slug))
  val.email
)

;; Infix version: ;;
guess_name = val -> val.name | (val.slug & (`@` + val.slug)) | val.email

;;
### Struct field shorthands

Because key-values are paired via `=`, we can unambiguously support
key-only shorthands, similar to other languages:
;;
one = 10
two = 20
pair = {one two}

;;
Equivalent to the code below. Note how the syntax is completely identical
between variable and field assignment. This eases learning and refactoring.
;;
pair = {
  one = 10
  two = 20
}

;;
### Variables

The term "variable" (or "constant") typically refers to standalone symbols,
bound to some value while not being part of an explicitly declared structure.
Since [#scopes_are_structs], variables are struct fields, as covered above.

One quirk peculiar to standalone symbols is implicit dereferencing. Every
symbol references something. Using that symbol tends to yield whatever it
is referencing.

The implicit dereference can be "countered" with explicit referencing,
which yields _the actual variable_, which is a reference to a memory
location. In addition, the dot `.` connects the references implicitly:

  some_var.&
  some_var.some_field.&    -- Goes back to `some_var`.

Regular variable access is equivalent to a proc `S -> A` where S is a literal
symbol. This means we could potentially allow to define such a proc explicitly,
overriding the default behavior of symbol access, or providing a fallback. This
could allow library or app code to insert custom logic into variable / field
access for a given scope. Could be useful for embedded DSLs.
;;

;; Reading a variable calls `S -> A` ;;
some_variable

;;
### Constants

All "variables" and fields are immutable by default.
Mutability is opt-in via [#pragmas].

There are multiple kinds of constants:
- Compile-time constants.
- Execution-time constants.
- Execution-time pseudo-constants.

Compile-time constants are either hardcoded primitive literals, or arbitrary
values calculated by compile-time procs. The resulting value can be assigned
to any variable, but is immutable, inlined into the executable, and on most
systems stored in read-only memory.

Execution-time constants have a value computed at runtime. They can't be
reassigned, and the compiler rejects any attempts to modify the memory
they reference.

Execution-time pseudo-constants can't be reassigned, but the compiler
allows to modify the memory they reference.
;;

;;
### Overloads

Our flexible, extensible design inevitably requires overloads. The most basic
example is integer arithmetic. Adding `Int8` vs `Int64` requires distinct CPU
instructions, yet we want to write both with `+`. Most languages handle such
differences by stuffing them deep into the compiler core, as special cases.
We instead implement them as overloads, also known as specializations.

Overloads are only needed for procs. Using them for types and values easily
leads to ambiguities, and is not recommended. However, because we have the
"dump stuff into scope" import style, supporting overloads for all kinds of
constants or variables may be unavoidable, with contextual disambiguation
based on type information, falling back on explicit namespacing.

Since procs are unary, they are resolved by the first operand, then by the
second operand, and so on. We don't plan to support overloading on the return
type, because this interferes with type inference. Every proc is uniquely
identified by its symbol (if named) and inputs, but not by its output:

  A -> B -> C              -- identified by `A -> B`
  A -> B -> C -> D         -- identified by `A -> B -> C`

  S -> A -> B -> C         -- identified by `S -> A -> B`
  S -> A -> B -> C -> D    -- identified by `S -> A -> B -> C`

Proc declarations in sub-scopes can shadow procs coming from super-scopes,
but conflicting declarations within one scope are not allowed.

All proc declarations, whether they're overloads or overrides, are lexically
scoped. Types don't carry procs along. A data type for which no procs are in
scope is just a chunk of bytes with a known size. This is true even for
"built-in" "primitives" such as integers.

It seems likely that we'll need to support a form of specificity in overload
resolution. Type constraints which are more specific need to take priority
over type constraints which are less specific.
;;

;;
## Refs

We intend to have generic references, similar to Rust's trait `Deref` and
Clojure's `IDeref`. We call it `Ref`. Any `Ref A` is an opaque value which
implicitly dereferences to `A` on demand.

Not to be confused with reference types, which is a distinct concept.
See [#type_system] for those.

References must be non-nilable by default.
Nilability / optionality should be opt-in.

`Ref` implementations may be either read-only, or read-write. A read-write
`Ref A`, in addition to implicitly dereferencing to `A` on a read, also
accepts `A` on a write. A writable ref is a `MutRef`.

The most notable refs are pointers. The language should have automatic memory
management without GC with ownership tracking, in the spirit of Swift and
Nim. There may be multiple strategies. We might end up with exclusively
owning pointers like Rust `Box`, and also with multi-ownership pointers
like Rust `Rc`. All of them would be `Ref`.

Refs are not limited to pointers. For example, a reactivity / observability
library would provide observable references. Their implementation would be
opaque, they could be "fat pointers" or something even fatter. By virtue of
being `Ref`, they dereference to underlying values on demand.

Concurrent code involves futures (`Fut`). It seems likely that a `Fut A` is
also a `Ref A`. Clojure does this. A future's eventual value or error would
be stored and reused for every dereference. Futures could also be lazy.

A `Ref` is not necessarily a _reference type_. The concepts are distinct. `Ref`
is just a interface / trait. Some refs, such as smart pointers, are unavoidably
reference types, but many could be value types. This should be transparent /
invisible to code using them.
;;

;;
Conceptually, every variable of `A` is a read-write `Ref A`. Since scopes are
structs and variables are fields, this also applies to struct fields. Fields
are refs. However, variables and fields are shy, reticent refs: they like to
implicitly dereference. To mutate one, its reference must be taken firmly:
;;
val = some_struct.some_field   ;; `A` ;;
ref = some_struct.some_field.& ;; `Ref A` ;;
val = ref.*                    ;; `Int` ;;
ref = 123                      ;; Assigns the field. ;;
some_struct.some_field.& = 123 ;; Identical to the above. ;;

;; Also works for nested structs: ;;
ref = struct_0.struct_1.field_0.&
ref = 123
log.(struct_0.struct_1.field_0) ;; 123 ;;

;; Accessing a list element returns a reference: ;;
list = [10 20 30]
ref = list.1.&
ref = 40
log.(list) ;; [40 20 30] ;;

;;
Immediate procs which implement conditionals might end up returning references:
`A.Ref`. Dereferencing the result evaluates the conditional.

These refs should exist in memory only at compilation time, not at program
execution time. Furthermore, they should be compatible with compile-time
constant evaluation and dead branch elimination.
;;
opt =     predicate  ?     expression              ;; A.Opt.Ref -> A.Opt ;;
opt = if.(predicate).then.(expression)             ;; A.Opt.Ref -> A.Opt ;;
val =     predicate  ?     branch0   !    branch1  ;; A.Ref     -> A     ;;
val = if.(predicate).then.(branch0).else.(branch1) ;; A.Ref     -> A     ;;

;;
## Named parameters

Complicated APIs tend to grow over time, adding parameters / options.
Sometimes variants can be disambiguated with overloads, sometimes they
can't. One nice solution is to encourage named parameters.

Most languages support only one form of proc calls, where parameters and
arguments are a heterogeneous tuple. This easily leads to horrible APIs,
readily apparent from a look at OS syscalls in any C standard library.

To keep complex APIs maintainable while allowing evolution without breakage,
we encourage named parameters and arguments. Some languages already take this
approach; the most notable current example is Swift, which inherited this
from Objective-C, which inherited this from Smalltalk. Swift more or less
enforces named arguments. It also treats them as a separate concept from
structs. We will simply use structs.

We should make it very easy to declare an inline anonymous struct type as an
input to a proc, and very easy to use it inline at callsites.

Because we have overloads, APIs can evolve over time. A proc can start, for
example, by taking a string. Later it adds an overload that takes a URL.
Eventually it defines the "full" configurable version that takes a struct,
and redefines the older variants to call the full one.

Sample definition below. The exact syntax is TBD.
;;
some_proc =
  ;; The proc takes one input: an inline struct. ;;
  {
    arg0 = type0
    arg1 = type1
    arg2 = type2
  }
  ->
  ;;
  The proc's output: a block. The last expression is the result.
  The compiler infers its type.
  ;;
  (
    some_expr
    some_expr
    some_expr
    some_result
  )

;; Calling this proc: ;;
some_proc.{
  arg0 = val0
  arg1 = val1
  arg2 = val2
}

;; For complex APIs, we also encourage named _return_ parameters via structs: ;;
some_proc = {
  inp0 = type0
  inp1 = type1
} -> (
  out2 = some_computation
  out3 = some_computation
  ;; The result is an inline struct. The compiler infers the shape. ;;
  {
    out2 = out2
    out3 = out3
  }
)

;; Many procs will have non-struct-taking overloads for simple cases. ;;
file = fs.open.`readme.md`

;; Another overload would take a file URL. ;;
file = fs.open.(some_file_url)

;; The fully configurable version takes a struct. ;;
file = fs.open.{
  path   = `readme.md`
  read   = true
  write  = true
  create = true
}

;;
## Modules and imports

- Modules are blocks.
- Blocks are structs.
- Modules are structs.
- Blocks are sub-modules.

Every module is a struct whose type is inferred by the compiler.
Declarations in module root are its "fields".

It should be possible to "import" a struct.

It should be possible to declare a sub-"module" as a block or struct.

We could make modules "callable", by defining anonymous procs `M -> A -> B`,
where M is the inferred type of a particular module, and A is an input type.
Such procs could be defined either inside or outside that module.

Three import styles are planned:
- Named import: explicit namespacing.
- Anon import: dump into scope, privately.
- Include: dump into scope, publicly.
;;

;; Named import ;;

str = import.`std:strings`

str.join_lines.[`one` `two` `three`]

str.join.{
  src = [`one` `two` `three`]
  sep = `_`
}

str.split.{
  src = `one two three`
  sep = ` `
}

;;
Anon import.

Basically dumps an entire module into the current scope. The dump is private,
not exported. However, identifiers visibly declared in the current module take
priority over "invisible" identifiers from that module. When importing in a
sub-scope, declarations from lexical super-scopes override conflicting ones
from the imported module.
;;

import.`std:strings`

join_lines.[`one` `two` `three`]

join.{
  src = [`one` `two` `three`]
  sep = `_`
}

split.{
  src = `one two three`
  sep = ` `
}

;; Include-style import. Same as anon import, but the dump is public, too. ;;
include.`std:strings`
;; All the same stuff. ;;

;;
### Module-level overrides

Part of the reason for treating modules as values, is to allow overrides.

Let's say the module `std:paths` implements path operations. It defines a
module-level constant `SEP = "/"` and uses that throughout the module.

Now let's say someone is writing code for the RISC OS, where FS paths denote
root with `$` and separate directories with `.`.

User code needs `SEP` to be `"."`, and probably needs to override procs such
as `is_root` and `is_abs` to respect the root prefix `$`. The rest of the
code in `std:paths` should respect the overrides. As long as the module is
carefully written to support such overriding, by not hardcoding separators,
the rest of it should "just work".

The simplest way of supporting overrides is probably via module inheritance.
A user declares `riscos/paths` which inherits from `std:paths` by dumping
everything in, include-style, and declares its own constants and procs as
overrides. The compiler respects this by copying the original procs and
specializing them for the new module.
;;

include.`std/paths`
SEP = `.`
is_root = val -> val.has_prefix.`$` ;; Override. ;;
;; ...Other proc overrides as needed. ;;

;;
Since modules are structs, and blocks are structs,
this should be doable with a block:
;;
mod = struct.(
  include.`some_other_module`
  some_proc = some_override_proc
)
mod.some_proc

;;
### Import path resolution

Modules are files, and imports are file-oriented: you don't import a package,
you import a specific file. This applies to standard library modules, local
modules, and external library modules.

Imports use protocols, similar to URL protocols. The most important ones are
`std:` and `git:`. The set of supported protocols is likely to grow over time.

Initially we'll require explicit file extensions. This has many advantages:
- Ease of learning: makes it totally clear where the system finds the module.
  Languages with "magic" package resolution are hostile to beginners.
- Ease of implementation: the system doesn't need to do any "magic".
- Friendly to network imports: a file is resolved unambiguously in a single
  network request. This is also why the ES module system, designed for browsers,
  requires explicit file extensions.
- Avoid ambiguity when supporting imports for different file types.

To clarify the latter, if we treated `./some_mod` as `./some_mod.mox`,
and also had support for `.json` imports, there would be an ambiguity
between `./some_mod.json` and `./some_mod.json.mox`.

Once the system is stable, we may consider supporting implicit file extensions
and package entry files, similar to how it's done in the JS world.
;;

;; Stdlib imports: ;;
os = import.`std:os.mox`
io = import.`std:io.mox`

;; Local imports use explicitly relative paths or absolute paths: ;;
some_util = import.`./util.mox`
some_lib = import.`/Users/me/some_private_lib/some_file.mox`

;;
For libraries, piggybacking on Git seems like a good start.
Go and Swift did that to great success:
;;
toml = import.`git:example.com/some_author/toml/index.mox`

;;
HTTPS imports have been very successful in the JS world.
There are many CDNs serving library code. For us, this
seems irrelevant for native modules, but could be useful
for JS interop (importing JS libs).
;;
some_lib = import.`https://example.com/some_lib/index.mox`
some_lib = import.{
  src  = `https://example.com/some_lib/index.mjs`
  type = `js`
  decl = {
    SOME_CONST = Str
    some_proc  = Some_type -> Other_type
  }
}

;;
Finally, the "real" case. Any real project wants to declare specific dependency
versions in its config. This also allows to give them names and avoid having
to use URLs in imports.

Something very similar is supported by Swift, but perhaps JS is the most
famous example. Both have used this to great success.

Config and usage:
;;
{
  deps = {
    lib_0 = {
      type = `git`
      repo = `https://example.com/author_0/lib_0.git`
      tag  = `0.1.0`
      hash = `d5c4e6b776d64add84143bf54a0e2713` ;; Auto-inserted ;;
    }
    lib_1 = {
      type = `git`
      repo = `https://example.com/author_1/lib_1.git`
      tag  = `0.2.0`
      hash = `b6b25b7cc6b44521aa539a1141444936` ;; Auto-inserted ;;
    }
  }
}
lib_0 = import.`lib_0/some_file.mox`
lib_1 = import.`lib_1/another_file.mox`

;;
### C interop: import and export

Play nicely with the industry-standard ecosystem.

We want to be able to easily import C-like libraries. Depending on our bootstrap
strategy, this could be delayed, but may also be needed immediately for the
language itself.

We'd like to automatically generate bindings from header files. This involves
preprocessing, parsing, and understanding C. Preprocessing could be done by
shelling out to a C compiler, but this doesn't help us with parsing, and we
don't want any "shelling out" to begin with. The practical solution is probably
to integrate libclang or cparser, which can handle all these steps, and give us
access to the AST for bindings generation. Ironically this requires explicit
bindings upfront.

We should probably print auto-generated bindings to a file, preferably
preserving the original comments. This lets the users actually see the
definitions they're working with. In addition, this serves as caching,
so we don't need to regenerate bindings when there's no change.

For non-shared libraries, the preferred way is probably to explicitly provide
header and object paths, as relative paths since they're likely to be vendored.
Seamless interop should look roughly like this:

  some_lib = import.{
    type   = `c`
    header = `./vendor/some_header.h`
    object = `./target/some_object.o`
  }

In addition to import, we want to support export. We'd like to auto-generate
C-style libraries: nicely commented header files + object files. This requires
translation from our semantics to C semantics, the exact inverse of the import
translation, and needs to respect the C ABI.

Auto-generation of headers was solved for Rust via `cbindgen`, which could be
a useful example for us: `https://github.com/mozilla/cbindgen`.

Supporting export also automatically makes the language embeddable, since the
compiler (which is a library!) can then be imported by a C/C++ app. Existing
projects could embed this for "scripting" without taking a performance hit,
once we meet our performance goals.

Interop with C++ is trickier; it's unclear if we can ever implement that
other than via C wrappers (import) and printing out C++ code (export).
;;

;;
Since automatic generation of bindings is not available at first, we'll need
the ability to declare them. This means respecting the C ABI. Many languages,
including Rust, Nim, Zig, and many more, make special provisions for C in their
type systems, basically replicating the C system and requiring its use in FFI.
We'll have the exact same headaches.

In binding declarations, we could apply some of those provisions, such as struct
packing, automatically. In addition, the compiler would require that all types
are either C types, or types which are C-compatible and map exactly to some C
types. The latter should be true for all fixed-size numbers supported in C, as
well as ordinal enums.

We provide conversion procs for all C types, mapping them to our types.
Free or very cheap conversion is implicit, expensive conversion explicit.
;;
some_lib = import.{
  type = `c`
  obj  = `./target/some_object.o`
  decl = {
    SOME_CONST = Uint32
    some_proc  = [C.string C.int] -> C.int
  }
}
[ptr len]   = `some_string`.cstring_pair
result_code = some_lib.some_proc.[ptr len]

;;
### Import types

We should ship with at least two import types:
- Import language code.
- Import a file as a chunk of bytes.

Being able to import arbitrary bytes is handy for a variety of cases. It allows
libraries to keep data in separate files next to code, but when imported, make
it available programmatically. It allows apps to bundle assets straight into an
executable, simplifying distribution. When the compiler runs in watch mode, it
should watch imported files and update modules on changes to those files.

Go ended up adding support for embedding arbitrary files in a slightly awkward
way, which involves special support in the compiler, special pragmas in user
code, and having to import a special pseudo-package. They still considered it
worth doing. Clearly use cases exist.

The Go embed approach is interesting in that it supports directories.
It lets you choose between importing a blob and importing a virtual
file system. One of the benefits is not having to hardcode every file
path. If we're serious about embedding, we'll have to do the same,
but the implementation can be delayed.

The likely approach is to define an interface (let's say `Importer`) supported
by the `import` proc, which allows to define the import behavior for any type,
by implementing that interface for a type. It would be invoked at import time.
The implementation of this interface for `Str` and `[Uint8]` is to simply
read the target file and return the bytes as-is. The implementation of this
interface for `FS` (a virtual file system) is to inspect the file system on
disk and read all the the directories and files, storing the data in memory
to be eventually included into the executable.

Finally, we may consider supporting inline-pluggable decoders.
Could be handy for importing structured data such as JSON or TOML.
Our own data notation would be supported by default, without needing
to provide a decoder.
;;
some_text = import.{
  src  = `./some_text.txt`
  type = Str
}
some_blob = import.{
  src  = `./some_blob.bin`
  type = Uint8.Arr
}
tzdata = import.{
  src  = `./iana_timezone_data`
  type = FS
}
some_config = import.{
  src  = `./config.mox`
  type = Config
}
some_config = import.{
  src    = `./config.json`
  type   = Config
  decode = json.decode
}

;;
## Catch-all procs

We may want the ability to define catch-all procs which match any type on
some of their inputs. This could be useful for traits. A trait could provide
a default implementation for all types at once, with special overloads for
specific types.

This raises an issue of proc overload specificity and how to decide
overload priority, but we already have that with regular overloads.
;;

;;
## Conditionals

Like most of the language, conditionals should be defined in the standard
library as immediate procs. The compiler core shouldn't have to know anything
about them. External libraries should be able to define more.

These are rough sketches. We have infinite possibilities.

See [#refs] for an explanation on how "partially formed"
conditionals could possibly work.
;;

;; Variadic `and`: ;;
and.(one two)
and.(one two three)
and.(one two three four)

;; Variadic `or`: ;;
or.(one two)
or.(one two three)
or.(one two three four)

;; Binary `and`: ;;
one & two
one & two & three
one & two & three & four

;; Binary `or`: ;;
one | two
one | two | three
one | two | three | four

;;
C-style ternary conditionals seem viable, but they
can't use `:` because that's the pragma operator.
;;
val = predicate ? branch0            ;; A.Opt.Ref -> A.Opt ;;
val = predicate ? branch0 ! branch1  ;; A.Ref     -> A     ;;

;; More verbose conditionals could be implemented as well. ;;
opt = if.(predicate).then.(expression)             ;; A.Opt.Ref -> A.Opt ;;
val = if.(predicate).then.(branch0).else.(branch1) ;; A.Ref     -> A     ;;

;; Multi-line: ;;
val = if.(predicate)
  .then.(branch_then)
  .else.(branch_else)

;;
A Lisp-style / lambda-calculus-style "if/then/else" should be easy to implement.
Convenient to write, but less clear.
;;
val = choice.(
  predicate
  branch_then
  branch_else
)

val = case.(some_value).{
  `possible_value_0` = branch_0
  `possible_value_1` = branch_1
  `possible_value_2` = branch_2
  _                  = branch_default
}

val = case.{
  predicate_0 = branch_0
  predicate_1 = branch_1
  predicate_2 = branch_2
  _           = branch_default
}

val = case.{
  when = some_predicate
  then = branch_then
  else = branch_else
}

case.{
  when = some_value
  then = log.`truthy`
  else = log.`falsy`
}

;;
Since we allow duplicate keys in {} literals, we could easily support this
keyword-style syntax as well. Like all other special expressions, this would be
done entirely in (standard) library code, not in compiler core. External
libraries can invent their own alternatives. That said, stdlib conditionals
are likely to get special support from syntax highlighters.
;;
case.{
  when = one   then = two
  elif = three then = four
  elif = five  then = six
  else = seven
}

;;
Some languages support `?.` or `.?` for optional chaining. We should be able to
support this in library code, for "optional" types, by implementing `?` as a
proc which discards the subsequent expression if the LHS is nil. Our version
would be marginally more verbose, but not intrinsic to the compiler, and
implementable for arbitrary library types:
;;
some_optional.?.some_proc
some_optional.?.some_proc.another_proc
some_optional.?.some_proc.`some_input`

;;
### Loops

We provide the typical "while" and "iterate this collection" loops.
TCO-based recursion is not planned; if ever implemented, it would
require explicit opt-in.

It needs to be possible to implement the "iterate me" interface / protocol
on arbitrary types. For regular dense arrays, this needs to have zero
runtime overhead.

Loops are expressions. A "break" terminates the loop, optionally returning
a provided value, if any. The result can be assigned to a variable.

The syntax is entirely undecided.
;;

;; Syntax is TBD. ;;
mut : ind = 0
while.(predicate).(
  val = some_coll.(ind)
  ind.++
)
result = while.(predicate).(
  val = some_coll.(ind)
  val == some_sentinel ? val.break
  ind.++
)

;; Syntax is TBD. ;;
coll.iter.[val].(
  log.[`val: ` val]
)
coll.iter.[val key].(
  log.[`val: ` val]
  log.[`key: ` key]
)
coll.iter.[_ key].(
  log.[`key: ` key]
)

;;
## Type system

The current plan is to start with a type system similar to Nim's. Most likely,
we'll make various alterations that suit our needs better. Swift's system is
also another nice candidate to pilfer from.

Implementation of types not needed for compiler bootstrapping can be delayed.
It also depends on how we bootstrap. The following is roughly in priority order,
but not exactly.

- Machine words.
- Other integers (various bit widths).
- Floats (compiler doesn't need them until user programs do).
- Fixed size arrays.
- Variable-size lists (slices / vectors).
- Procs (funcs).
- Reference types (non-nil by default).
- Structs.
- Enums.
- Misc common data structures, most importantly dicts and sets.
- First-class types (compile-time reflection).
- Inheritance (it's not just for "objects").
- Generics (see [#generics]).

Most of this requires no special explanation.

One special note is that we aim to keep most type implementations in library
code and out of compiler core. It should be possible for external libraries
to implement types which feel just as first-class as the stdlib types. For
example, it should be possible to implement fixed-precision decimals which
feel just like native integers and floats, with implicit conversion from
literals, operators, etc.

Haskell / GHC may provide examples of keeping built-in types in library code.
We should look into that.

### Reference types

Some types and behaviors are unavoidably referential.

Prioritize safety, then performance:
- References are not nil.
- Nil is opt-in and always checked.
- No unsafe memory sharing by default (Rust's `Send`).
- Implicit copy on write for immutable types.

Most languages use a mix of value types and reference types.
This is even true for heavily object-oriented languages:
- Smalltalk never truly went for "everything is a unique object".
- Java never truly went for "everything is a unique object".

Other observations:
- Computers like to copy small data and to reference big data.
- C did okay with just 1 reference kind (pointer).
- Go shipped with 6 reference kinds.
- Swift shipped with around 3 reference kinds.

Use cases:
- Dumb pointers.
- Smart pointers.
- Resizable data structures.
- Procs / funcs.
- Concurrency primitives.
- Other forms of shared memory.

Non-negotiables:
- Automatic memory management.
- Nil safety.

Swift may be a nice role model here. Reference types are non-nil,
and nilness is an explicit type-level opt-in with visible checks.
Additionally, immutable structures have built-in copy on write.

For reference types which are inherently nilable, e.g. pointers,
we can treat all-zero as nil, but make this opt-in, unlike Go.

### Distinct types

Make it possible to declare a distinct type of any other type.

The new type inherits the memory layout of the old type.
The two are not _implicitly_ convertible to each other.

This warrants a comparison with Go, where a distinct type always inherits all
language-level built-in operations, and is explicitly convertible to the base
type, but does not inherit any methods. This works out quite well in practice
and is very useful.

Our aims are different:
- Keep "built-ins" out of compiler core.
- Define most types and procs in stdlib.
- No "operators", just procs.
- No "methods", just procs.

So the natural solution is that distinct types inherit everything, but require
explicit conversion. Distinctness is the only difference.

We could also consider supporting a form of selective distinctness where only
specific behaviors are inherited, but only after looking at actual use cases.

### Conversions

Basic no-brainers:
- Only safe conversions by default.
- Pointers are distinct from numbers.
- Distinct types require explicit conversion.
- Allow implicit widening conversions where safe.
- No overflow/underflow by default. Numeric conversions can fail.

Basic aims:
- Make safe cheap conversions implicit.
- Make expensive conversions explicit.
- Prefer compile-time computations in release builds.

We want to make it possible to define custom library-level comtime conversions
between arbitrary types. The use cases are endless:
- Implicitly converting numeric literals / constants to "built-in" numeric
  types. Most languages build this into compiler core; we'd like to keep
  this in library code.
- Implicitly converting integers to "big ints" and floats to "big floats".
  Sub-case of implicit widening conversions. Constant values should be
  converted at comtime in release builds, dynamic values at runtime.
- Implicitly converting strings to URLs. Constant values should be parsed
  at comtime in release builds.
- Implicitly converting strings to file path constructs. Constant values should
  be parsed at comtime in release builds.
- Parsing embedded SQL at comtime in release builds.
- Implicitly converting `Str -> Rune`, which saves on syntax.
- Implicitly converting constant lists to sets at comtime.

### Implicit vs explicit typing

We're targeting two extremes and everything in-between. Extremes:
- Entirely duck-typed code relying on static type inference. Good for scripting
  and high-level business lotic. Can lead to obtuse type errors when overused.
- Entirely explicitly-typed code. Should be used in libraries and utility code.
  More verbose, but provides more guarantees and makes type errors nicer.

The default is extreme type inference (see Haskell),
suitable for scripting and high level code.

For library code and large complicated projects, where duck typing can be too
error prone or the errors become too obtuse, it should be possible to enforce
explicit-only typing at the package level, via its config file:

  {
    name   = `enterprise_monorepo`
    typing = `explicit`
  }

### Generics and traits

In this context, "generic" means "type-parametrized".
We'll use the former for brevity.

Figuring out a generic type system for a new language takes years, even with
many role models to yoink from. For now, we only list basic aims:
- Support generic procs.
- Support generic types.
- Support traits (interfaces).
- Prioritize static dispatch.
- Boxed / dynamic traits are an explicit opt-in at callsites,
  but not in proc declarations.
- Support "duck-typed" generic code.
- Support fully explicit generic code.

Start by investigating existing implementations. Interesting examples include
Nim, Swift, Go, and Zig amond others.

Go launched without type parametrization, relying entirely on dynamic dispatch
for generic code. It implicitly converts concrete values to interface fat
pointers, and implicitly moves / copies values to the heap when necessary.
This easily leads to high GC overheads, and many years of research went into
mitigating that. At the time of writing this doc, they're looking into
rewriting the garbage collector to be friendlier to modern hardware.
We aim to avoid GC in the first place, and our generics should be statically
dispatched by default; boxed traits should be opt-in.

Swift did something interesting with witness tables and type metadata,
which avoids over-specialization, but comes with runtime overheads.
This was followed by years of research into how to speed that up,
and where to specialize instead.

We can delay the implementation of boxed traits / interface objects. Many of
their use cases are covered by enums, and the rest can be emulated.
First-class support may be added later, with an explicit opt-in.

We should ensure that generic procs and types written to expect a trait `T` can
be used with both concrete and boxed / dynamic `T`. Regular generics shouldn't
have to be specially modified for interface objects. This already works in Go:
every interface has its own special concrete type which implements that same
interface, so it works with generic funcs too.

Our duck-typed generic code will probably be somewhat similar to Zig generics,
Rust `macro_rules`, or Nim templates. Relevant read:
- https://typesanitizer.com/blog/zig-generics.html

However, it should also be possible to specify constraints explicitly,
for better error messages and maintainability. It should also be possible
to enforce explicit generic constraints at package level:

  {
    name            = `enterprise_monorepo`
    type_parameters = `explicit`
  }

In explicit type parameter declarations, it would be nice to avoid having to
specify types twice, which is sadly very common. Go example:

  func someFunc[T fmt.Stringer](val T) string {
    return val.String()
  }

Simply taking a trait / interface should be equivalent to specifying that
you take any concrete type conforming to that trait. The compiler should
be able to specialize this proc:

  (fmt.Stringer -> Str) :
  some_func = val -> val.string

When the underlying concrete type is needed,
there should be a simple way to retrieve it:

  (fmt.Stringer -> Str) :
  some_func = val -> log.[`concrete type: ` val.concrete_type]

Additional note. Over-specialization of generic code can bloat compilation
time and executable size. Slim binaries are not on our priority list, but
compilation times are, so this may require care.

### Classes and subclasses

Our real-world needs for generic code and polymorphism should be well-covered
by a combination of: static type parametrization for procs and types; traits;
enums; typedef inheritance (see [#distinct_types]); proc overloads.

Long-lived object-oriented languages sometimes acquire all of these concepts.
Ironically, this obviates object orientation in the first place, by replacing
it with a collection of orthogonal concepts which compose better.

Have you ever noticed how subtype polymorphism is redundant with interfaces?
Interfaces are the extracted essence of OO polymorphism without the need for
subtyping.

Interop with C++ should not require any "classes" as the C ABI tends to be
considered the lingua franca, and major C++ libraries provide C interfaces.

Interop with JS and DOM requires support for emitting code with classes and
methods. One notable example is the custom DOM element API, which is useful
for optimizing hybrid SSR/SPA webapps and/or making webapps easier to write.
Custom elements must be subclasses of DOM element classes, and some of their
behaviors are only available through lifecycle callbacks defined as methods.

Medium priority later, lower priority at first. We aim for customizable codegen
(whether machine or textual). This should be solvable on a case-by-case basis
or in library code, while keeping classes out of the type system. Clean support
should be possible to add later.
;;

;;
### Custom logic in type implementations

Since we aim to keep most of the language outside the compiler core, this
requires the ability for library code to define types and operations on
types imperatively, after reflecting on the inputs, making specialized
optimizations for specific types, etc.. Most languages stuff all this
into the compiler.

One interesting example is tagged unions. They need space for the tag.
Some compilers go to great lengths to avoid wasting space by reusing
bits which are otherwise unused. Sometimes it can be found in padding.
Sometimes even inside of the provided types.

Suppose we're declaring an `Opt` of `Bool`. Conceptually, it's a union of
`Nil` and `Bool`: `Nil | Bool`. Some languages would force an additional
wrapper here (`Some Bool`), but that should be avoidable. Since the union
has 2 variants, it needs 1 bit to represent which variant is active. The
type `Bool` also uses only 1 bit for its state, but on modern computers,
it takes a whole whopping byte (8 bits) since the hardware doesn't support
addressing smaller chunks. The implementation of the hypothetical type kind
`Union` would reflect on `Bool` and find out that it's an ordinal enum whose
highest value doesn't use the whole chunk of memory it consumes. As a result,
it would generate a union type which uses one of those "dead" bits for its
tag, and consumes no extra memory. Presumably it would pick a low bit rather
than a high bit, and declare its bit width in type metadata in the same way
as it was declared for `Bool`. That would make it stackable several layers
deep within the same byte with no extra logic.

Again, this is done in many languages. But such power should not be kept locked
inside chunky monolithic compilers, but rather in a modular fashion available
to libraries.
;;

;;
### Type instantiation syntax

Types (and type kinds) generally use `Title_snake_case` naming.

Instantiation of generic types uses the regular "call" syntax.
No special syntax is introduced here.

Generic types with only one mandatory parameter can be instantiated
by being "called" with that parameter as-is.

  Str.List    -- list of string

Fixed-size byte array (compiler intrinsic for stdlib);
size is arbitrary:

  Memory.4
  Memory.8
  Memory.16
  4.Memory    -- also valid

Pointer / reference:

  Int.Ptr
  Str.Ref

Dynamically-sized generic array (length is tracked separately):

  Int.Arr
  Str.Arr

Fixed-size generic array:

  Arr.[4 Int]
  Arr.[4 Str]

Similarly to our preference that procs should use named parameters, in the form
of a struct, we may end up requiring named type parameters, even though it's
notably clunky in the simple cases of 2 or 3 inputs:

  Dict.{
    key = Int
    val = Str
  }

Open question: explicit type-parametrization of procs.
;;

;;
## Concurrency and IO

Threads and synchronous IO will be supported in any case. However, highly
concurrent programs, especially servers, want to use async IO, which allows
for much higher network throughput and moderately higher FS throughput.

The code should not require any changes between the two IO styles. In other
words, trying to use the result of an IO operation should implicitly block
the current routine, with an option to opt out of blocking and get a future
instead. We do not want the problem of "function color".

Let's say all IO operations, regardless of IO implementation, return futures.
Every `A.Fut` is also an `A.Ref` which _implicitly_ dereferences to A wherever
A is expected. See [#refs] for more. Code which doesn't need to use a future's
value can simply pass that future to other code which does.

Centering around async IO also plays nicely with the JS compilation target,
where it's the default anyway.

Async IO requires custom lightweight concurrency. The area is well-researched
and other languages provide plenty of examples of implementation strategies.
We'll aim for a balance between efficiency and debuggability (traces).

Full stack traces must be available in dev / debug modes, regardless of how
coroutines are implemented (i.e. "stackless"). Importantly, a trace should not
break off where one coroutine launches another. A coroutine's trace should be
conjoined to its ancestors. This is already implemented in major JS engines and
is valuable in debugging. This must not impede performance in fully optimized
release builds. There should be an option to compile with and without trace
support, and with trace support, there should be an option to enable or disable
them at runtime.

The same should be true when using OS threads.
Each child thread's trace should reference the parent's trace.
;;

;;
## Stack traces

Language should come with built-in support for capturing and displaying stack
traces, enabled by default in dev / debug builds. Release builds should have
an option to compile with or without trace support. The latter produces
smaller and faster executables which also don't leak filesystem paths. When
built with trace support, tracing should be togglable at runtime, and have
as little runtime cost as possible. Also, filesystem paths in release builds
should be relativized.

Traces are mainly used for errors, but sometimes it's useful
to simply capture and print a trace to see how we got there.

Also, not all errors need traces. See below.
;;

;;
## Errors

Errors should be:
- Informative.
- Efficient.
- Chainable.
- Customizable.

The language should have an `Error` interface / trait implementable by
arbitrary types, and provide a variety of tools for making errors easy.

Library code can usually get away with trace-free errors. Application code
should have a very easy way, ideally transparent, to opt into traces. When
tracing is enabled, calling a library proc and receiving an error should
automatically capture a trace at that point, wrapping the error with a
tracing-error type.

Traces should not be redundant. It should be possible to inspect an error and
detect whether it has a trace. This could be part of the `Error` trait, with
a default nop implementation automatically inherited by concrete types.

Error messages are separate from traces. We encourage informative error
messages, ideally actionable upon by final users who see them.

Errors should be stack-allocated wherever possible.

Errors should include context, such as which inputs caused the failure, what
was the path of a file which is not found, and so on. The preferred way is to
define an error type as a struct, and implement the `Error` trait by writing
a proc which formats its error message appropriately.

Many error types have context but are also recoverable, which means their
contents are often discarded. In such cases, the message will simply not be
generated.

The language should provide an easy way to chain errors and inspect error
chains. When writing a custom error type, there should be an easy way to
make it able to reference another error.

A transcript of an error's trace should include traces
of the ancestor errors which caused it, if any.

### Exceptions

To be usable for high-level business logic and scripting,
the language supports exceptions and uses them by default.

In proc signatures, it should be possible to declare exception types,
but this is not required by default.

### Strict mode

It would also be ideal to provide an opt-out from exceptions. It should be
possible to configure the language, at module level and at package level, to
have guaranteed local control flow, where non-local flow does not exist. For
now we'll call this "strict mode". It has the following effects:
- Control flow is explicit and guaranteed.
- Macros which secretly alter control flow cannot be called by strict code,
  either directly or indirectly.
- All procs in strict code must declare exception types. This may include
  memory allocation errors, although that could be rather unfortunate.
- Exceptions are auto-converted to error values.
- Procs with undeclared exceptions can't be called.
- All errors must be handled.

The compiler should be able to check if a proc declares all possible exception
types, by following its dependency tree, and enforce this for strict code.

Strict code can safely call exception-based code, as long as all exception
types are declared. When an exception reaches strict code, the language hands
the exception to that code and stops unwinding the stack. This also allows
libraries to be written in exception-based style while still being callable
by strict code. To make it easier to write libraries, there should also be a
midway mode between strict and loose, which requires exception declarations,
but uses exceptions for control flow. It should be configurable at the
package level and recommended by default for libraries.

Ideally, the language would come with a code formatter tool, and such a tool
could also modify code in semantically useful ways, spiritually similar to
`goimports` (which formats _and_ fixes imports). Such a tool could insert
exception declarations into proc signatures automatically. Formatters are
easier to integrate into code editors than LSPs. If we eventually get an LSP,
it will also provide this.

The ability to declare multiple exception types requires language-level
support for tagged unions, also known as sum types or variant types.

A union of errors should be considered an `Error`. This is automatic if
every union auto-implements every trait implemented by all member types.
;;

;;
The language should provide convenient ways of annotating errors.
For example (semi-pseudocode):
;;
(Person_id -> Person) :
get_person = id -> (
  ;;
  Automatically deferred. If an exception occurs and matches
  `db.err_not_found`, the exception is annotated with this
  additional message, which is prepended to the message of
  the wrapped error.
  ;;
  Error.only.(db.err_not_found).detail.[`unable to find user ` id]

  ;;
  Since this is the last expression, its result is
  implicitly returned from this proc.
  ;;
  Person : db.conn.query.[`select * from persons where id = $1` id]
)

;;
Setting exceptions and control flow aside, we may consider other forms of
strictness. In the end, instead of one overarching strict mode, we'll probably
end up with multiple compiler options which enable or disable those forms.
Package configuration could look like:
;;
{
  enforce_types         = true
  explicit_control_flow = true
}

;;
## Pragmas

Many kinds of declaration metadata are unavoidable:
- Types of symbols.
- Signatures of procedures.
- Private or public.
- Constant or mutable.
- Other context-sensitive modifiers, like `volatile`.
- Optional compiler hints.
- Call convention modifiers.
- Wrapping procs for debugging.
- Experimental language features.
- Library-specific annotations (JSON field names, etc.).

Many languages end up with loads of dedicated syntax, keywords, and so on.
We aim to solve all metadata needs with pragmas.

Pragmas assign metadata to AST nodes, which is inspected by the compiler and
various immediate procs which operate on the AST, such as `=` and `->`.

Tentatively (subject to revision), this is done via the immediate proc `:`.
The proc is right-associative and stackable. It's prepended to declarations:
;;
pub : some_var = `some_value`

;; Type is public, fields are private: ;;
pub : Some_type = struct.{
  one = Int
  two = Int
}

;; Type _and_ fields are public: ;;
pub : Some_type = pub : struct.{
  one = Int
  two = Int
}

;; More specialized: ;;
pub : Some_type = struct.{
  pub : one = Int ;; Public, immutable. ;;
  mut : two = Int ;; Private, mutable. ;;
}

;; Mutable local variable: ;;
mut : some_var = `some_val`

;; Public mutable variable (unsynchronized ): ;;
pub : mut : some_var = `some_val`

;; Could we make pragmas reusable? ;;
Model = Pragma.{
  pub         = true
  derive      = [derive.Debug derive.Show]
  json_fields = json.camel
  sql_fields  = sql.snake
}

;; Reuse the `Model` pragma. ;;
Model :
Person = struct.{
  id   = Uuid
  name = string
}

;; Reuse the `Model` pragma. ;;
Model :
Session = struct.{
  id        = Uuid
  person_id = Uuid
}

;;
Pragmas could be RHS-oriented with a dedicated operator.
Unsuitable for variables (makes syntax highlighting harder),
but could be useful elsewhere.
;;
some_int :: Int = 123

;; How to define module-level pragmas? Maybe anonymously. ;;
_ :: {
  vis = `package`             ;; Visible only in current package. ;;
  pub = [Some_type some_proc] ;; Declare exports in one place. ;;
}

;; We might be able to support pragma/field grouping: ;;
Parser = struct.{
  pub : (
    source   = Stream
    comments = Bool
  )
  mut : (
    position = Int
    peek     = Buffer
  )
}

;;
Worth reiterating that none of this involves special syntax.
This just uses a regular immediate proc (bound to an operator)
which attaches metadata to the AST nodes it receives.
;;

;;
## Math

Many arithmetic operations have edge cases which require special handling.
For those operations, the language should provide multiple procs with
different behaviors. Examples below.

Addition variants:
- Silent overflow.
- Overflow panics / returns an error (see [#exceptions] and [#strict_mode]).
- Overflow caps, returning the maximum value.

Similar for subtraction and multiplication.

Division variants:
- Division by zero returns 0. (Conveniently free on some architectures.)
- Division by zero panics.

Floating point operations:
- IEEE 754 compliant variants.
- Variants which panic on NaN or infinity.
- Variants which truncate Infinity to the maximum representable value,
  except for division by zero which panics. (Posits behave similarly.)

NaN should probably be equal to NaN.

Recent languages disagree on arithmetic. In Swift, overflow kills your entire
program and cannot be caught . Swift also provides multiple variants of
arithmetic operators, letting your opt into mathematically incorrect behaviors
such as overflows. Meanwhile, in Go, integers overflow by default, requiring
tricky extra library code with complicated test coverage just to get numeric
conversions which aren't mathematically insane, while at the same time, float
to integer conversions are best-effort and capping. Given the disagreements and
inconsistencies, there seems to be no default that satisfies every use case.
We aim to provide options and make the choices easy to enforce.

Overflow-capping produces mathematically "incorrect" results, yet is very common
in game logic, where it's often the correct way of doing arithmetic. A currency
counter or status effect counter should cap instead of overflowing, which does
happen a lot. This shows that "correctness" in arithmetic is multi-faceted.

In actual application code, we want to make it easy to use the semantically
correct arithmetic without having to remember to insert extra bounds checks
or which operators to choose. Making this configurable at the package level
is possible, but realistically, many packages will want to assign different
arithmetic behaviors to different variables / fields, in a controllable yet
mix-and-match way. Doing this at the type level seems like the most natural
centerpiece. We provide several variants of each numeric type, differing in
how they handle arithmetic operators.
;;

;;
### Truthiness

There's "some" and "truthy".

A value is considered "some" when it's not nil. Non-nilable types are
automatically "some". Nilable types do a runtime check.

A value is considered "truthy" if automatic conversion to `Bool` is
implemented for its type and the result is `true`. This also applies
to `Bool` since every type is automatically convertible to itself.

`Nil` may be auto-convertible to `Bool`, in which case the result
will always be `false`.

We don't define other automatic conversions to `Bool`.
User code should be free to define its own.

Conditional forms such as `if` and `case` take `Bool` in predicate positions.
Types which auto-convert to `Bool` can be passed in those positions without
explicit conversion.
;;

;;
## SIMD

It would be nice to provide SIMD-compatible "vector" numeric types, such as
Uint128 and Uint256, and emit SIMD instructions for operations on them, on
supported architectures, with regular arithmetic as a fallback.
;;

;;
## Fast iteration

One of the aims of this project is to eliminate waiting-for-compilation
for arbitrarily large codebases, up to a point anyway.

Large codebases are full of features, but rarely all the features are used or
tested at once. In development, you typically work on one feature at a time
and want to test changes immediately, without a long compilation step.

A large codebase written in a compiled language doesn't have to feel like a
sluggish, chunky monster. It should be possible to make a compiled-language
codebase feel as nimble as a modular, mostly lazily-imported scripting
language with HMR which respects developer time.

This should be achievable through the following features:
- JIT compilation in development.
- Lazy code loading in development.
- Native watch mode in development (small deal compared to other two).
- Statically linked AOT in production for the exact same code.

### Native JIT

Tiered JITs were invented decades ago; see the Self language. The most
prominent current example is JS. Compiled languages should learn from
browser JS engines, which prioritize good balance between low latency
and high throughput by using tiered JITs. This balance also happens
to be perfect for developer workflows. We need a language which combines
JIT compilation with AOT compilation and makes it transparent to switch
between the two.

Side note: the Swift REPL seems to use JIT compilation, but the language
sadly does not provide a full-featured option to use JIT when developing
actual apps. Such a miss, especially on MacOS where this is sorely needed.

Another note in favor of hybrid AOT + JIT compilation. At the time of writing,
MacOS runs security checks on every new executable. On large binaries this
can take _seconds_, wasting huge amounts of developer time and slowing down
iteration. This is particularly time-wasting for languages which statically
link by default, such as Go, generating larger executables. This can only be
disabled system-wide, hugely weakening system security. A compiled language
with JIT support would skip this time-waster by running generated code in its
own process, instead of writing an executable to disk and invoking it, which
is currently the default in compiled languages.

### Lazy code loading and analysis

Lazy loading means importing only the modules which are _actually_ currently
needed. We could take two approaches to this: explicit and implicit.

The explicit approach can be seen in Python and JS, where imports can be moved
from module root to inside of functions and executed on demand. This is often
used in web applications in combination with routing; the net result is that
when an application grows in size by adding new routes, development restart
times stay nearly constant. Similarly, when adding a large new dependency
which is only used occasionally, importing it lazily avoids affecting the
common case of when it's unused, both in development and in production.

The implicit approach is to automatically convert module-level imports into
local imports, at the language level. This involves adding runtime checks
such as "is this module imported"; it should be possible to keep them nearly
negligible. When building an executable in release mode, we transparently
switch to eager imports and skip this altogether.

Naturally, this can be done with dylibs, which have been around for decades.
But in prior compiled languages, making a codebase modular by splitting it
into multiple dylibs takes extra work. This needs to be a built-in feature
which reduces development latency at zero extra work from developers.

Another neat trick used by JS engines is to avoid analyzing code which has
not been called, even in modules which are actually loaded. Statically typed
languages would see this as anathema: what do you mean, _don't_ analyze the
code? But unused code is unused; this lets us run _used_ code sooner. The
compiler could do both: prioritize JIT compilation and execution of actually
used code, then use spare resources to finish analysis in the background.

Ignoring unused definitions is not just for development speed. The compiler
should also do this in release builds. It's sadly very common to import a
huge library, like an SDK for some cloud API provider, only to use a tiny
portion of it. The compiler should avoid analyzing unused stuff and
tree-shake it out of the app. This should make compilation faster and
resulting executables smaller.

This "lax mode" also extends to "errors" in used code which don't actually
affect execution, such as unused variables, unused imports, unused expression
results. They should be ignored in development.

Sidenote: modules are files, not directories. In languages where directories
are modules, such as Go, it's far too easy to accidentally end up with a
bloated "main" module which takes forever to compile and is impractical to
split up at that point. Files have the right level of modularity and
naturally lend themselves to caching and lazy loading.

Implicitly lazy imports, if we're able to pull them off, would also be excellent
for bundle splitting and async imports in the JS compilation target.

### Native watch mode

Having a native watch mode allows the compiler to keep context in RAM and
avoid repeatedly reading, parsing, analyzing code, speeding up iteration.

This also potentially opens the door to HMS: hot module swapping, typically
called HMR these days (hot module reload). At the time of writing, HMS has
been available in Erlang for almost 40 years, yet still no major compiled
language ships with HMS support.

### Hot module swapping

In prior compiled languages, HMS is sometimes implemented with dylibs.

At the time of writing, Unreal Engine 5 has 2 different implementations of
HMS. But this doesn't ship with C++; only UE users get that experience,
which has plenty of caveats anyway.

Nim 2 ships with its own HCR, implemented in 666 lines of code with comments.
It also uses dylibs. It has limitations such as being unable to reload the
main module of the program. The compiler doesn't ship with any kind of watch
mode, which is needed for full HMS.

Compiled languages should ship with full support for HMS.
This feature should not be limited to specific frameworks.

### Plugins and mods

Native support for JIT + dynamic code loading + watch + HMS should enable
something extremely useful: plugins and mods, implementable in the native
language of the given application (not an embedded scripting language),
distributable as source code (not as architecture-specific dylibs),
updatable at application runtime, runnable and swappable in a production
app by both plugin/mod authors and users.

The desire for plugins and mods is pervasive. Media software, games, code
editors, web frameworks and content management systems, complex web platforms
such as Wikipedia, compilers and package managers, and more.

In interpreted languages, loading code is trivial, but compiled languages
force either an embedded VM for a scripting language, or using dylibs for
plugins. Both have _major_ issues.

Embedding a scripting language splits developer brains, often adds the costs
of encoding and decoding between different binary representations of data,
prevents proper AOT compilation to machine code, and makes plugins and mods
significantly slower than they should be. While some scripting languages can
be AOT compiled to bytecode, this does not solve brain split, data encoding
and decoding, and inferior performance of interpreted code. Note that some
target platforms, notably iOS, also forbid shipping a JIT compiler with apps.
Which means that embedded scripts must either stay forever interpreted and
thus slow, or be properly AOT compilable to machine code (not VM bytecode),
which is not supported by any embeddable VM the author knows of.

Using dylibs means that plugins have to be distributed as precompiled binary
blobs which are specific to a platform and architecture. This is even worse
than an embedded scripting language. Plugin authors have to compile blobs for
each platform and architecture supported by the app, and end users have to be
software reverse engineering experts to find out if the blob secretly steals
their SSH keys and so on. Which leads to the next point.

Being able to load arbitrary plugin source code, rather than dylibs, is great
for audit and security, but nowhere near enough. Nobody has the full combo of
time, inclination, and competence to validate the source code of every single
program they download before running it.

Which leads us to sandboxing, which should have at least two levels:
process-level and plugin-level.

We can't fully sandbox external code at the level of the standard library,
because it can arrange for arbitrary syscalls on its own.

Some operating systems support somewhat fine-grained sandboxing of process
level IO. An attempt to access an un-pledged resource either causes an error
or terminates the app outright. At the time of writing, this is available in
OpenBSD more easily, in Linux less easily, and sadly missing from other
operating systems. See https://justine.lol/pledge/.

Ideally, an app should be able to fine-tune its IO constraints for mods and
plugins without needing special OS support. At the time of writing, WASM and
WASI comes to mind, as a balance of speed and security. WASM is a sandbox by
design. Syscalls are not available, and by extension, neither is IO. This of
course is full of tradeoffs which impede performance.

Conclusion so far. An ability to load arbitrary _application-native_ code at
runtime, with JIT compilation, seems hugely desirable for its combination of
usability, performance, and auditability. We also want to be able to sandbox
external code's IO without a significant performance hit.
;;

;;
## Memory management

To keep developers productive, memory management must be automatic.

Because of soft-realtime use cases, such as games, memory management
must be deterministic.

We'll probably use an approach similar to Nim, with ownership tracking,
simple automatic alloc/free for single-owned values, and reference counting
for shared values. This is only done for reference types and avoided for value
types. Reference counting also needs cycle detection; we could borrow ideas
from Nim's ORC. Relevant reading:
- https://nim-lang.org/blog/2020/10/15/introduction-to-arc-orc-in-nim.html

We could also borrow ideas from Mojo. Instead of reference counting shared
values by default, it assigns a single owner to each reference, does allow
sharing the reference with other code, and ensures that the owner outlives
the borrowers. This gets you the equivalent of reference counting without
the actual counting when owners and lifetimes are clear. It also provides
`ArcPointer` for when they're unclear. Relevant reading:
- https://docs.modular.com/mojo/manual/values/
- https://docs.modular.com/mojo/manual/values/ownership
- https://docs.modular.com/mojo/manual/lifecycle/death/

When compiling for the JS target, we simply leave this to the GC.

A lofty goal for later it to support pluggable allocators. This is relevant
for some performance-sensitive programs, such as games. We should keep an
eye on our APIs to keep them compatible with this.
;;

;;
## Destructors

We should support destructors. In the native target,
they should be deterministic.

In the native target, destruction is done either inline when owner lifetime
is clear, or via ARC which invokes a destructor at zero.

In the JS target, because we don't use reference counting, we would like to
use `FinalizationRegistry`. It's non-deterministic and often massively
delayed, but for many use cases that involve destructors which aren't about
memory, this doesn't bother us. We could also resort to ARC for references
with custom destructors, and avoid it for regular references, which is most
of them.

When using `FinalizationRegistry`, destructors can't receive the reference as
an input; the API tries to prevent programmers from accidentally retaining the
value. So if we take this route, our destructors have to be about fields.
Instead of writing a destructor like a method, which takes an entire struct,
it would be written for a specific field of that struct. Seems fiddly, and
perhaps we should just use reference counting in those edge cases involving
multiple-ownership.
;;

;;
## Libraries

At first, the language should draconically forbid packages from importing
other packages. Only applications can import packages.

Reasons:
- Encourages contributing to the standard library.
- Minimizes unnecessary dependencies.
- Minimizes redundant dependencies.
- Minimizes library tree bloat.
- A little copying is better than a little dependency.

Eventually the restriction may be relaxed. But maybe not.

Recent languages and package managers tend to use semantic versioning for
packages. Dependency versions are often pinned. Any popular library that
receives a lot of attention will frequently release new versions, and many
other libraries will depend on different versions of it. Then the tree
converges in some application which ends up importing like a bazillion
redundant copies of that library, some of which would actually be compatible.
This bloats executables and compilation times.

Instead, we encourage contributions to the standard library.
Apps will have an easier time and less bloat.

If we relax this in the future, we will allow only 1 major version of any
given package in a dependency tree, choosing the highest one. Different
major versions would be unfortunate, but allowed. Packages are encouraged
to increment the major version ONLY for breaking changes. Major new features
without breaking changes are not an excuse for a major version increment.

All this said, libs should be allowed to depend on other libs for testing.
Execution-time deps, compile-time deps, testing-time deps, are separate.
Libs should be free to use arbitrary testing-time deps, and apps should be
free to _not_ download or run any of that.
;;

;;
## Language versioning

The language should aim for maximum backward compatibility.
Code that builds and runs now, should build and run forever.

Every package declares its preferred language version in its config.
The language tooling adds this to the config automatically if missing.

Breaking changes in the language and standard library should be minimized as
much as possible. But when a certain breaking change is unavoidable or highly
desirable, it should apply only to packages which declare that version of the
language (or higher) as their preferred version. The old behavior should be
kept in the compiler and applied to all packages whose preferred language
version is lower.
;;
